{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import sys, six\n",
    "\n",
    "# Import Data Sccience libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Instantiates a client\n",
    "client = language.LanguageServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_suicide_df = pd.DataFrame.from_csv(\"sample_suicide_watch_text.csv\",encoding = \"ISO-8859-1\").iloc[0:10,:]\n",
    "\n",
    "small_suicide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_text(text):\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    sentiment = client.analyze_sentiment(document).document_sentiment\n",
    "    \n",
    "    return [sentiment.score,sentiment.magnitude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_sentiment_text(text):\n",
    "    \"\"\"Detects entity sentiment in the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "        content=text.encode('utf-8'),\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        print('Mentions: ')\n",
    "        print(u'Name: \"{}\"'.format(entity.name))\n",
    "        for mention in entity.mentions:\n",
    "            print(u'  Begin Offset : {}'.format(mention.text.begin_offset))\n",
    "            print(u'  Content : {}'.format(mention.text.content))\n",
    "            print(u'  Magnitude : {}'.format(mention.sentiment.magnitude))\n",
    "            print(u'  Sentiment : {}'.format(mention.sentiment.score))\n",
    "            print(u'  Type : {}'.format(mention.type))\n",
    "        print(u'Salience: {}'.format(entity.salience))\n",
    "        print(u'Sentiment: {}\\n'.format(entity.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mentions: \n",
      "Name: \"words\"\n",
      "  Begin Offset : 57\n",
      "  Content : words\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.3393266499042511\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"misconception\"\n",
      "  Begin Offset : 199\n",
      "  Content : misconception\n",
      "  Magnitude : 0.800000011920929\n",
      "  Sentiment : -0.800000011920929\n",
      "  Type : 2\n",
      "Salience: 0.19303403794765472\n",
      "Sentiment: magnitude: 1.600000023841858\n",
      "score: -0.800000011920929\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"anyone\"\n",
      "  Begin Offset : 330\n",
      "  Content : anyone\n",
      "  Magnitude : 0.30000001192092896\n",
      "  Sentiment : -0.30000001192092896\n",
      "  Type : 2\n",
      "Salience: 0.09136639535427094\n",
      "Sentiment: magnitude: 0.699999988079071\n",
      "score: -0.30000001192092896\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"attention\"\n",
      "  Begin Offset : 144\n",
      "  Content : attention\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.09088879823684692\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"pain\"\n",
      "  Begin Offset : 218\n",
      "  Content : pain\n",
      "  Magnitude : 0.20000000298023224\n",
      "  Sentiment : -0.20000000298023224\n",
      "  Type : 2\n",
      "Salience: 0.05292052775621414\n",
      "Sentiment: magnitude: 0.20000000298023224\n",
      "score: -0.20000000298023224\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"feelings\"\n",
      "  Begin Offset : 240\n",
      "  Content : feelings\n",
      "  Magnitude : 0.6000000238418579\n",
      "  Sentiment : -0.6000000238418579\n",
      "  Type : 2\n",
      "Salience: 0.04622408002614975\n",
      "Sentiment: magnitude: 0.6000000238418579\n",
      "score: -0.6000000238418579\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"assumption\"\n",
      "  Begin Offset : 188\n",
      "  Content : assumption\n",
      "  Magnitude : 0.10000000149011612\n",
      "  Sentiment : -0.10000000149011612\n",
      "  Type : 2\n",
      "Salience: 0.04348456487059593\n",
      "Sentiment: magnitude: 0.10000000149011612\n",
      "score: -0.10000000149011612\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"feelings\"\n",
      "  Begin Offset : 477\n",
      "  Content : feelings\n",
      "  Magnitude : 0.20000000298023224\n",
      "  Sentiment : -0.20000000298023224\n",
      "  Type : 2\n",
      "Salience: 0.033725421875715256\n",
      "Sentiment: magnitude: 0.20000000298023224\n",
      "score: -0.20000000298023224\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"persistence\"\n",
      "  Begin Offset : 444\n",
      "  Content : persistence\n",
      "  Magnitude : 0.8999999761581421\n",
      "  Sentiment : -0.8999999761581421\n",
      "  Type : 2\n",
      "Salience: 0.029522333294153214\n",
      "Sentiment: magnitude: 0.8999999761581421\n",
      "score: -0.8999999761581421\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"ideations\"\n",
      "  Begin Offset : 486\n",
      "  Content : ideations\n",
      "  Magnitude : 0.10000000149011612\n",
      "  Sentiment : -0.10000000149011612\n",
      "  Type : 2\n",
      "Salience: 0.029522333294153214\n",
      "Sentiment: magnitude: 0.10000000149011612\n",
      "score: -0.10000000149011612\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"thoughts\"\n",
      "  Begin Offset : 468\n",
      "  Content : thoughts\n",
      "  Magnitude : 0.4000000059604645\n",
      "  Sentiment : -0.4000000059604645\n",
      "  Type : 2\n",
      "Salience: 0.029522333294153214\n",
      "Sentiment: magnitude: 0.4000000059604645\n",
      "score: -0.4000000059604645\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"myself.\"\n",
      "  Begin Offset : 519\n",
      "  Content : myself.\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.020462526008486748\n",
      "Sentiment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "entity_sentiment_text(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data = []\n",
    "\n",
    "for record in small_suicide_df.iterrows():\n",
    "    text = record[1][\"text\"]\n",
    "    \n",
    "    extra_record = sentiment_text(text)\n",
    "    extra_data.append(extra_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data_df = pd.DataFrame(extra_data, columns=[\"Score\",\"Magnitude\"],index=small_suicide_df.index)\n",
    "\n",
    "extra_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_combined_df = small_suicide_df.join(extra_data_df)\n",
    "\n",
    "small_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "depression_df = pd.DataFrame.from_csv(\"depressionTrainingSet3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep500_df = depression_df.iloc[0:500,0:1].reset_index()\n",
    "\n",
    "dep500_extra = []\n",
    "\n",
    "for record in dep500_df.iterrows():\n",
    "    text = record[1][\"text\"]\n",
    "    \n",
    "    extra_record = sentiment_text(text)\n",
    "    dep500_extra.append(extra_record)\n",
    "\n",
    "dep500_extra_df = pd.DataFrame(dep500_extra,columns=[\"sentiment\",\"magnitude\"])\n",
    "\n",
    "dep500_extra_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_dep500_df = dep500_df.join(dep500_extra_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = big_dep500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 2)\n"
     ]
    }
   ],
   "source": [
    "nodepression_df = pd.DataFrame.from_csv(\"depressionTrainingSet4.csv\")\n",
    "nodep500_df = nodepression_df.iloc[0:500,0:1].reset_index()\n",
    "\n",
    "nodep500_extra = []\n",
    "print(nodep500_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for record in nodep500_df.iterrows():\n",
    "    text = record[1][\"text\"][0:999]\n",
    "    \n",
    "    extra_record = sentiment_text(text)\n",
    "    nodep500_extra.append(extra_record)\n",
    "\n",
    "nodep500_extra_df = pd.DataFrame(nodep500_extra,columns=[\"sentiment\",\"magnitude\"])\n",
    "\n",
    "nodep500_extra_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodep500 = nodep500_df.join(nodep500_extra_df)\n",
    "\n",
    "training_data = pd.concat([big_dep500_df, all_nodep500])\n",
    "\n",
    "training_data.to_csv(\"training_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start doing classification with SKlearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.loc[:,[\"sentiment\",\"magnitude\"]]\n",
    "y = training_data.loc[:,\"risk_of_suicide\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Score: 0.780\tMSE:-0.220\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression()\n",
    "\n",
    "score = cross_val_score(regr,X,y)\n",
    "mse = cross_val_score(regr,X,y,scoring=\"neg_mean_squared_error\")\n",
    "print(f\"Logistic Regression:\\nScore: {score.mean():.3f}\\tMSE:{mse.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-DTC with max_depth 02:\tScore:0.725\tMSE:-0.275\n",
      "B-DTC with max_depth 03:\tScore:0.707\tMSE:-0.293\n",
      "B-DTC with max_depth 04:\tScore:0.699\tMSE:-0.302\n",
      "B-DTC with max_depth 05:\tScore:0.705\tMSE:-0.290\n",
      "B-DTC with max_depth 06:\tScore:0.706\tMSE:-0.312\n",
      "B-DTC with max_depth 07:\tScore:0.702\tMSE:-0.301\n",
      "B-DTC with max_depth 08:\tScore:0.705\tMSE:-0.297\n",
      "B-DTC with max_depth 09:\tScore:0.690\tMSE:-0.300\n",
      "B-DTC with max_depth 10:\tScore:0.698\tMSE:-0.311\n",
      "B-DTC with max_depth 11:\tScore:0.708\tMSE:-0.296\n",
      "B-DTC with max_depth 12:\tScore:0.700\tMSE:-0.301\n",
      "B-DTC with max_depth 13:\tScore:0.699\tMSE:-0.292\n",
      "B-DTC with max_depth 14:\tScore:0.700\tMSE:-0.298\n",
      "B-DTC with max_depth 15:\tScore:0.704\tMSE:-0.297\n",
      "B-DTC with max_depth 16:\tScore:0.700\tMSE:-0.302\n",
      "B-DTC with max_depth 17:\tScore:0.697\tMSE:-0.289\n",
      "B-DTC with max_depth 18:\tScore:0.698\tMSE:-0.292\n",
      "B-DTC with max_depth 19:\tScore:0.707\tMSE:-0.304\n",
      "B-DTC with max_depth 20:\tScore:0.700\tMSE:-0.283\n",
      "B-DTC with max_depth 21:\tScore:0.711\tMSE:-0.291\n",
      "B-DTC with max_depth 22:\tScore:0.719\tMSE:-0.296\n",
      "B-DTC with max_depth 23:\tScore:0.688\tMSE:-0.292\n",
      "B-DTC with max_depth 24:\tScore:0.708\tMSE:-0.294\n",
      "B-DTC with max_depth 25:\tScore:0.697\tMSE:-0.284\n"
     ]
    }
   ],
   "source": [
    "for depth in range(2,26):\n",
    "    boosted_dtc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth))\n",
    "    boosted_dtc.fit(X,y)\n",
    "    currScore = cross_val_score(boosted_dtc,X,y)\n",
    "    currMSE = cross_val_score(boosted_dtc,X,y,scoring=\"neg_mean_squared_error\")\n",
    "    print(f\"B-DTC with max_depth {depth:02d}:\\tScore: {currScore.mean():.3f}\\tMSE: {currMSE.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_sentiment(text):\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "            content=text.encode('utf-8'),\n",
    "            type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    salience_ls = [[] for x in range(8)]\n",
    "    score_ls = [[] for x in range(8)]\n",
    "    magnitude_ls = [[] for x in range(8)]\n",
    "\n",
    "    for entity in result.entities:\n",
    "        salience_ls[entity.type].append(entity.salience)\n",
    "        score_ls[entity.type].append(entity.sentiment.score)\n",
    "        magnitude_ls[entity.type].append(entity.sentiment.magnitude)\n",
    "\n",
    "    mean_salience_ls = [sum(x)/len(x) if x else 0 for x in salience_ls]\n",
    "    mean_score_ls = [sum(x)/len(x) if x else 0 for x in score_ls]\n",
    "    mean_magnitude_ls = [sum(x)/len(x) if x else 0 for x in score_ls]\n",
    "\n",
    "    all_ls = mean_salience_ls + mean_score_ls + mean_magnitude_ls\n",
    "    return all_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_training_data = []\n",
    "for record in training_data.iterrows():\n",
    "    text = record[1][\"text\"][0:999]\n",
    "    \n",
    "    extra_record = get_entity_sentiment(text)\n",
    "    extra_training_data.append(extra_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_ls = [\"UNKNOWN\",\n",
    "      \"PERSON\",\n",
    "      \"LOCATION\",\n",
    "      \"ORGANIZATION\",\n",
    "      \"EVENT\",\n",
    "      \"WORK_OF_ART\",\n",
    "      \"CONSUMER_GOOD\",\n",
    "      \"OTHER\"\n",
    "     ]\n",
    "\n",
    "ls = [\"salience\",\"sentiment\",\"magnitude\"]\n",
    "\n",
    "together = []\n",
    "\n",
    "for x in ls:\n",
    "    for y in type_ls:\n",
    "        together.append(y+\"_\"+x)\n",
    "\n",
    "extra_training_data_df = pd.DataFrame(extra_training_data,columns=together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.index = extra_training_data_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 28)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_training_data = training_data.join(extra_training_data_df)\n",
    "extended_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should yield better performance\n",
    "\n",
    "X = extended_training_data.loc[:,\"sentiment\":]\n",
    "y = extended_training_data.loc[:,\"risk_of_suicide\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-DTC with max_depth 02:\tScore: 0.746\tMSE: -0.253\n",
      "B-DTC with max_depth 03:\tScore: 0.734\tMSE: -0.270\n",
      "B-DTC with max_depth 04:\tScore: 0.780\tMSE: -0.240\n",
      "B-DTC with max_depth 05:\tScore: 0.752\tMSE: -0.239\n",
      "B-DTC with max_depth 06:\tScore: 0.763\tMSE: -0.238\n",
      "B-DTC with max_depth 07:\tScore: 0.753\tMSE: -0.241\n",
      "B-DTC with max_depth 08:\tScore: 0.778\tMSE: -0.247\n",
      "B-DTC with max_depth 09:\tScore: 0.751\tMSE: -0.253\n",
      "B-DTC with max_depth 10:\tScore: 0.755\tMSE: -0.261\n",
      "B-DTC with max_depth 11:\tScore: 0.746\tMSE: -0.255\n",
      "B-DTC with max_depth 12:\tScore: 0.763\tMSE: -0.246\n",
      "B-DTC with max_depth 13:\tScore: 0.759\tMSE: -0.248\n",
      "B-DTC with max_depth 14:\tScore: 0.747\tMSE: -0.242\n",
      "B-DTC with max_depth 15:\tScore: 0.755\tMSE: -0.238\n",
      "B-DTC with max_depth 16:\tScore: 0.755\tMSE: -0.237\n",
      "B-DTC with max_depth 17:\tScore: 0.757\tMSE: -0.250\n",
      "B-DTC with max_depth 18:\tScore: 0.741\tMSE: -0.266\n",
      "B-DTC with max_depth 19:\tScore: 0.747\tMSE: -0.256\n",
      "B-DTC with max_depth 20:\tScore: 0.753\tMSE: -0.257\n",
      "B-DTC with max_depth 21:\tScore: 0.727\tMSE: -0.273\n",
      "B-DTC with max_depth 22:\tScore: 0.731\tMSE: -0.251\n",
      "B-DTC with max_depth 23:\tScore: 0.751\tMSE: -0.258\n",
      "B-DTC with max_depth 24:\tScore: 0.745\tMSE: -0.268\n",
      "B-DTC with max_depth 25:\tScore: 0.742\tMSE: -0.268\n"
     ]
    }
   ],
   "source": [
    "for depth in range(2,26):\n",
    "    boosted_dtc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth))\n",
    "    currScore = cross_val_score(boosted_dtc,X,y)\n",
    "    currMSE = cross_val_score(boosted_dtc,X,y,scoring=\"neg_mean_squared_error\")\n",
    "    print(f\"B-DTC with max_depth {depth:02d}:\\tScore: {currScore.mean():.3f}\\tMSE: {currMSE.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Score: 0.779\tMSE:-0.221\n"
     ]
    }
   ],
   "source": [
    "regr = LogisticRegression()\n",
    "regr.fit(X,y)\n",
    "score = cross_val_score(regr,X,y)\n",
    "mse = cross_val_score(regr,X,y,scoring=\"neg_mean_squared_error\")\n",
    "print(f\"Logistic Regression:\\nScore: {score.mean():.3f}\\tMSE:{mse.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 26)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5       ,  1.10000002,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [ 0.89999998,  0.89999998,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.69999999,  0.69999999,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.2       ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.56000001,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.56000001],\n",
       "       [ 0.30000001,  0.60000002,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.2       ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.24      ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.24      ],\n",
       "       [-0.1       ,  1.60000002,  0.        ,  0.80891061,  0.01684985,\n",
       "         0.        ,  0.0055462 ,  0.        ,  0.0055462 ,  0.02330673,\n",
       "         0.        ,  0.        , -0.2       ,  0.        , -0.40000001,\n",
       "         0.        , -0.1       , -0.34285714,  0.        ,  0.        ,\n",
       "        -0.2       ,  0.        , -0.40000001,  0.        , -0.1       ,\n",
       "        -0.34285714],\n",
       "       [-0.30000001,  1.5       ,  0.        ,  0.2615382 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.15897454,\n",
       "         0.        , -0.25      ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        , -0.3       ,  0.        , -0.25      ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        -0.3       ]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"I hate myself, I'm going to kill myself, this day is terrible. What am I doing with my life???\"\n",
    "text2 = \"What a beautiful day, I feel so happy!\"\n",
    "text3 = \"It is not a lack of love, but a lack of friendship that makes unhappy marriages.\"\n",
    "text4 = \"Whoever fights monsters should see to it that in the process he does not become a monster. And if you gaze long enough into an abyss, the abyss will gaze back into you.\"\n",
    "text5 = \"God is dead. God remains dead. And we have killed him. Yet his shadow still looms. How shall we comfort ourselves, the murderers of all murderers? What was holiest and mightiest of all that the world has yet owned has bled to death under our knives; who will wipe this blood off us? What water is there for us to clean ourselves?\"\n",
    "text6 = \"Today sure is a terrible day. My sister just died, and I lost my job. However, I will not let my depression take over, since this would left everyone i know in sorrow!\"\n",
    "\n",
    "text_ls = [text1,text2,text3,text4,text5,text6]\n",
    "# I would personally say 1,0,0,?,1\n",
    "\n",
    "data = np.array([sentiment_text(text) + get_entity_sentiment(text) for text in text_ls])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosted_dtc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4))\n",
    "boosted_dtc.fit(X,y)\n",
    "boosted_dtc.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
